{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import sqlite3\n",
    "import threading\n",
    "import numpy as np\n",
    "import requests as r\n",
    "from csv import writer\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_url = \" https://www.google.com/search?q=\"\n",
    "user_agent = {\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(name):\n",
    "    for i in name:\n",
    "        if i not in 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz1234567890_' :\n",
    "            j = name.index(i)\n",
    "            name = name[:j:]+name[j+1::]\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting search result urls and filter to get only the working and relevent links\n",
    "\n",
    "def search(item):\n",
    "    print(\"searching for\", item)\n",
    "    search_result = r.get(search_url+item)\n",
    "\n",
    "    if search_result.status_code != 200:\n",
    "        print(\"ERROR! Searching Failed\")\n",
    "        quit()\n",
    "\n",
    "    soup = bs(search_result.content, 'html.parser')\n",
    "    search_result = soup.find(\"div\", {\"id\": \"main\"})\n",
    "    links = search_result.find_all('a')\n",
    "\n",
    "    urls = []\n",
    "    print(\"Filtering for relevent URLs\")\n",
    "    for link in links:\n",
    "        if link['href'].startswith(\"/url?q=https://www.\"):\n",
    "        \n",
    "            url = link['href'].split(\"/url?q=\")[1].split('&')[0]\n",
    "            page = r.get(url, headers=user_agent)\n",
    "        \n",
    "            if page.status_code == 200:\n",
    "                title = ''\n",
    "                try:\n",
    "                    title = bs(page.text, 'html.parser').title.text\n",
    "                    if set(title.split()) & set([\"Lifelong\", \"LLPCM05\", \"Trimmer\"]):\n",
    "                        urls.append(url)\n",
    "                    else:\n",
    "                        print(\"1m - \", title)\n",
    "                except Exception as e:\n",
    "                    print(\"1\", title, e, link['href'])  \n",
    "\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv\n",
    "def dump_to_csv(rows):\n",
    "    np.savetxt(\"GFG.csv\", rows, delimiter =\", \", fmt ='% s', encoding = 'utf-8')\n",
    "\n",
    "def csv_dump(reviews):\n",
    "    print(\"creating csv dump\")\n",
    "    header = ['id', 'website', 'title', 'name', 'date', 'rating', 'r_text', 'address']\n",
    "    i = 1\n",
    "    with open(\"reviews.csv\", 'w', encoding = 'utf-8') as f:\n",
    "        write = writer(f)\n",
    "        write.writerow(header)\n",
    "        for row in reviews:\n",
    "            r = (i,) + row\n",
    "            write.writerow(r)\n",
    "            i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class amazon:\n",
    "    def __init__(self) -> None:\n",
    "        self.reviews: list\n",
    "        \n",
    "    def get_reviews(self, url):\n",
    "        soup = bs(r.get(url, headers=user_agent).text, \"html.parser\")\n",
    "        reviews = soup.find(\"div\", {\"id\":\"cm-cr-dp-review-list\"})\n",
    "        review_data = reviews.find_all(\"div\", {\"class\":\"a-section review aok-relative\"})\n",
    "\n",
    "        reviews = []\n",
    "        for review in review_data:\n",
    "            name = review.find(\"span\", {\"class\": \"a-profile-name\"}).text.strip()\n",
    "            rating = review.find('a', {\"class\": \"a-link-normal\"}).text.strip()\n",
    "            title = review.find('a', {\"data-hook\":\"review-title\"}).text.strip()\n",
    "            date = review.find('span', {\"data-hook\":\"review-date\"}).text.strip().split(\"on\")[1]\n",
    "            r_text = review.find(\"div\", {\"data-hook\":\"review-collapsed\"}).text.strip()\n",
    "            address = \"India\"\n",
    "            reviews.append((\"amazon\", title, name, date, rating, r_text, address))\n",
    "        self.reviews =  reviews\n",
    "\n",
    "class flipkart:\n",
    "    def __init__(self) -> None:\n",
    "        self.reviews: list\n",
    "\n",
    "    def get_reviews(self, url):\n",
    "        soup = bs(r.get(url, headers=user_agent).text, \"html.parser\")\n",
    "        reviews_data = soup.find_all(\"div\", {\"class\": \"col _2wzgFH\"})\n",
    "\n",
    "        reviews = []\n",
    "        for review in reviews_data:\n",
    "            name = review.find(\"p\", {\"class\":\"_2V5EHH\"}).text.strip()\n",
    "            rating = review.find(\"div\", {\"class\":\"_1BLPMq\"}).text.strip()\n",
    "            title = review.find('p', {\"class\":\"_2-N8zT\"}).text.strip()\n",
    "            date = review.findAll('p', {\"class\":\"_2sc7ZR\"})[1].text.strip()\n",
    "            r_text = review.find(\"div\", {\"class\":\"t-ZTKy\"}).text.strip()\n",
    "            address = review.find(\"p\", {\"class\":\"_2mcZGG\"}).text.strip().split(\", \")[1]\n",
    "            reviews.append((\"flipkart\", title, name, date, rating, r_text, address))\n",
    "        self.reviews = reviews\n",
    "\n",
    "class snapdeal:\n",
    "    def __init__(self) -> None:\n",
    "        self.reviews: list\n",
    "\n",
    "    def get_reviews(self, url):\n",
    "        soup = re.compile(\"\\\"bucketLabelNames.*?,\").findall(r.get(url, headers=user_agent).text)[0]\n",
    "        page = r.post(\"https://www.snapdeal.com/acors/web/getReviewsAndRatings/v2\", headers={\n",
    "                \"referer\": url,\n",
    "                \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\"\n",
    "            }, json={\n",
    "                \"productId\": url.split('/').pop(),\n",
    "                \"pageUrl\": url,\n",
    "                \"bucketLabelNames\":[\"appliances-personal1care1appliances-trimmers\"]\n",
    "                }\n",
    "        ).text\n",
    "\n",
    "        soup = bs(page, 'html.parser')\n",
    "        review_data = soup.find(\"div\", {\"class\":\"commentreview\"}).find_all(\"div\", {\"class\": \"text\"})\n",
    "\n",
    "        reviews = []\n",
    "        for review in review_data:\n",
    "            name_date = review.find(\"div\", {\"class\":\"_reviewUserName\"})\n",
    "            name = name_date[\"title\"]\n",
    "            date = name_date.text.strip().split(\"on\")[1].strip()\n",
    "            rating = len(review.find_all('i', {\"class\":\"active\"}))\n",
    "            title = review.find(\"div\", {\"class\": \"head\"}).text.strip()\n",
    "            r_text = review.find('p').text.strip()\n",
    "            address = ''\n",
    "            reviews.append((\"snapdeal\", title, name, date, rating, r_text, address))\n",
    "        self.reviews = reviews\n",
    "\n",
    "class lifelongindiaonline:\n",
    "    def __init__(self) -> None:\n",
    "        self.reviews: list\n",
    "        \n",
    "    def get_reviews(self, url):\n",
    "\n",
    "        soup = bs(r.get(url, headers=user_agent).text, \"html.parser\")\n",
    "        product_id = json.loads(soup.find(\"script\", {\"id\":\"__st\"}).text.split('=')[1][:-1])['rid']\n",
    "        page = r.post(f\"https://www.lifelongindiaonline.com/apps/aliexpress_reviews?product_id={product_id}&customer_id=\", \n",
    "            headers={\n",
    "                \"referer\": url,\n",
    "                \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\"\n",
    "            }\n",
    "        ).text\n",
    "\n",
    "        soup = bs(page, \"html.parser\")\n",
    "        review_data = soup.find(\"div\", {\"id\":\"wrapper\"}).find_all(\"div\", {\"class\":\"large-12 cell\"})\n",
    "\n",
    "        reviews = []\n",
    "        for review in review_data:\n",
    "            try:\n",
    "                d = {}\n",
    "                name = review.find(\"span\", {\"class\":\"areviews_user_name\"}).text.strip()\n",
    "                date = review.find('p', {\"class\":\"comment_time comment_time_theme2\"}).text.strip()\n",
    "                rating = len(review.find(\"span\", {\"class\":\"main_container_review_stars\"}).find_all('i', {\"class\":\"fas fa-star\"}))\n",
    "                title = ''\n",
    "                address = ''\n",
    "                r_text= review.find(\"div\", {\"class\":\"comment_container\"}).text.strip()\n",
    "                reviews.append((\"lifelongindiaonline\", title, name, date, rating, r_text, address))\n",
    "            except:\n",
    "                pass\n",
    "        self.reviews =  reviews\n",
    "\n",
    "class jiomart:\n",
    "    def __init__(self) -> None:\n",
    "        self.reviews = []\n",
    "    def get_reviews(self, url):\n",
    "        pass\n",
    "class croma:\n",
    "    def __init__(self) -> None:\n",
    "        self.reviews = []\n",
    "    def get_reviews(self, url):\n",
    "        pass\n",
    "class smartprice:\n",
    "    def __init__(self) -> None:\n",
    "        self.reviews = []\n",
    "    def get_reviews(self, url):\n",
    "        pass\n",
    "class reliancedigital:\n",
    "    def __init__(self) -> None:\n",
    "        self.reviews = []\n",
    "    def get_reviews(self, url):\n",
    "        pass\n",
    "class pricehunt:\n",
    "    def __init__(self) -> None:\n",
    "        self.reviews = []\n",
    "    def get_reviews(self, url):\n",
    "        pass\n",
    "class smartprix:\n",
    "    def __init__(self) -> None:\n",
    "        self.reviews = []\n",
    "    def get_reviews(self, url):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# database\n",
    "def create_databse():\n",
    "    con = sqlite3.connect('database.db')\n",
    "    cur = con.cursor()\n",
    "    cur.execute(\"CREATE TABLE if not exists reviewdb(id integer PRIMARY KEY AUTOINCREMENT, website varchar(15) NOT NULL, title varchar(50), name varchar(25), date varchar(20), rating integer, review_text varchar(100), address varchar(20))\")\n",
    "    con.commit()\n",
    "    con.close()\n",
    "    print(\"database created\")\n",
    "\n",
    "def insert(reviews):\n",
    "    print(\"Inserting reviews to database\")\n",
    "    con = sqlite3.connect('database.db')\n",
    "    cur = con.cursor()\n",
    "    print(\"inserting in database\")\n",
    "    cur.executemany(\"INSERT INTO reviewdb(website, title, name, date, rating, review_text, address) VALUES(?,?,?,?,?,?,?)\", reviews)\n",
    "    con.commit()\n",
    "    con.close()\n",
    "    return \"reviews inserted into database\"\n",
    "\n",
    "def fetch():\n",
    "    con = sqlite3.connect('database.db')\n",
    "    cur = con.cursor()\n",
    "    cur.execute(\"SELECT * FROM reviewdb\")\n",
    "    messages = cur.fetchall()\n",
    "    con.close()\n",
    "    return messages\n",
    "\n",
    "def show_reviews():\n",
    "    messages = fetch()\n",
    "    for message in messages:\n",
    "        print(message)\n",
    "\n",
    "def database_dump():\n",
    "    print(\"Creating database dump\")\n",
    "    con = sqlite3.connect('database.db')\n",
    "    with open(\"database_dump.sql\", 'w', encoding='utf-8') as f:\n",
    "        f.write(\"\\n\".join(con.iterdump()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_reviews(urls):\n",
    "    threads_list = []\n",
    "    website_objects = []\n",
    "\n",
    "    for url in urls:\n",
    "        website = validate(url.split('/')[2].split('.')[1])\n",
    "        print(f\"Fetching from - {website}, {url}\")\n",
    "        website_instance = eval(f\"{website}()\")\n",
    "        website_objects.append(website_instance)\n",
    "        t = threading.Thread(target = website_instance.get_reviews, args =(url,))\n",
    "        t.start()\n",
    "        threads_list.append(t)\n",
    "\n",
    "    for t in threads_list:\n",
    "            t.join()\n",
    "    for t in threads_list:\n",
    "        while t.is_alive():\n",
    "            continue\n",
    "\n",
    "    reviews = []\n",
    "    for webobject in website_objects:\n",
    "        if len(webobject.reviews):\n",
    "            reviews.extend(webobject.reviews)\n",
    "    \n",
    "    return reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to database and csv and create dump\n",
    "def main():\n",
    "    item = \"Lifelong LLPCM05 Beard Trimmer for Men\"\n",
    "    urls = search(item)\n",
    "    reviews = get_all_reviews(urls)\n",
    "    create_databse()\n",
    "    insert(reviews)\n",
    "    database_dump()\n",
    "    csv_dump(reviews)\n",
    "    print(\"COMPLETE !!!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "searching for Lifelong LLPCM05 Beard Trimmer for Men\n",
      "Filtering for relevent URLs\n",
      "1  'NoneType' object has no attribute 'text' /url?q=https://www.tatacliq.com/lifelong-llpcm05-cordless-rechargeable-beard-trimmer-black/p-mp000000005426190&sa=U&ved=2ahUKEwiXz9z1nqP6AhVIuJUCHenyANwQFnoECAQQAg&usg=AOvVaw3R6ehkRbbZxiJNL5qbsWhu\n",
      "Fetching from - amazon, https://www.amazon.in/Lifelong-LLPCM05-Beard-Trimmer-Cordless/dp/B07QGW4C1S\n",
      "Fetching from - amazon, https://www.amazon.in/Lifelong-LLPCM05-Cordless-Trimmer-Warranty/dp/B084XK89XH\n",
      "Fetching from - lifelongindiaonline, https://www.lifelongindiaonline.com/products/lifelong-llpcm05-beard-trimmeblack\n",
      "Fetching from - flipkart, https://www.flipkart.com/lifelong-llpcm05-trimmer-45-min-runtime-8-length-settings/p/itm59598a15aafc8\n",
      "Fetching from - snapdeal, https://www.snapdeal.com/product/lifelong-llpcm05-beard-trimmer-black-/675130634919\n",
      "Fetching from - reliancedigital, https://www.reliancedigital.in/lifelong-llpcm05-men-s-beard-trimmer/p/491903023\n",
      "Fetching from - jiomart, https://www.jiomart.com/p/electronics/lifelong-llpcm05-men-s-beard-trimmer/590042098\n",
      "Fetching from - croma, https://www.croma.com/lifelong-stainless-steel-blades-cordless-beard-trimmer-llpcm05-black-/p/228995\n",
      "Fetching from - smartprix, https://www.smartprix.com/trimmers/lifelong-llpcm05-trimmer-ppd1l77lygha\n",
      "database created\n",
      "Inserting reviews to database\n",
      "inserting in database\n",
      "Creating database dump\n",
      "creating csv dump\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "64a7f78d182328ff71a165ef4363fc1c80d667db642e192c6c627f8fa20df6ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
